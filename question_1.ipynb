{"cells":[{"cell_type":"markdown","metadata":{},"source":[" ![Université Laval](img/ulaval.jpg)\n"," # <center><b>GLO-4030/GLO-7030 : Apprentissage par réseaux de neurones profonds</b></center>\n"," # <center><b>Travail Pratique 2</b></center>\n"," ***\n"," __Course__: GLO-4030/GLO-7030 : Apprentissage par réseaux de neurones profonds <br>\n"," __Title__: Travail Pratique 2 <br>\n"," __Semester__: Winter 2021 <br>\n"," __Lecturer__: Dr. Pascal Germain <br>\n"," __Author__: Parham Nooralishahi <br>\n"," __Organization__: Université Laval <br>\n"," ***\n","\n"," ## Question 1 - Fine-tuning and Normalization (50%)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import parham_core as phm\n","from torchviz import make_dot\n","\n","import torchvision.models as models\n","import torchvision.datasets as ds\n","import torchvision.transforms as trans\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" |__Directory__|__Path__|\n"," |----------|-----|\n"," |__Dataset__|./datasets|\n"," |__Used Data__|./data_split|\n"," |__Images for the report__| ./img|"]},{"cell_type":"markdown","metadata":{},"source":[" ## Dataset Preparation & Processing\n"," In this question, you have to perform fine-grained classification of bird species. To do so, download the images of the dataset <a href=\"http://www.vision.caltech.edu/visipedia/CUB-200.html\">CUB-200</a> <br\\>.\n"," ### CUB-200 Dataset\n"," Caltech-UCSD Birds 200 (CUB-200) is an image dataset with photos of 200 bird species (mostly North American).\n"," * __Number Of Categories__: 200\n"," * __Number Of Images__: 6,033\n"," * __Annotations__: Bounding Box, Rough Segmentation, Attributes\n","\n"," @techreport{WelinderEtal2010, <br/>\n"," &emsp;&emsp; Author = {P. Welinder and S. Branson and T. Mita and C. Wah and F. Schroff and S. Belongie and P. Perona}, <br/>\n"," &emsp;&emsp; Institution = {California Institute of Technology}, <br/>\n"," &emsp;&emsp; Number = {CNS-TR-2010-001}, <br/>\n"," &emsp;&emsp; Title = {{Caltech-UCSD Birds 200}}, <br/>\n"," &emsp;&emsp; Year = {2010} <br/>\n"," } <br/>\n","\n"," ![CUB-200 Dataset](img/cub200.jpg)\n","\n"," ### Download and Unzip CUB-200 Dataset\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["orig_dataset_dir = 'orig_dataset'\n","orig_dataset_dir = phm.get_cub200(orig_dataset_dir)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Data Preparation (Training & Testing Subset)\n"," For each class, we sort the images in ascending order by file name and use the first 15 images as a test set. Then, we use the other images for training."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_dir='dataset'\n","training_ds_dir, testing_ds_dir = phm.prepare_subsets(orig_dataset_dir, dataset_dir)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Data Normalization\n"," Experiments are conducted twice with two different data normalization strategies. The first time, use the values from the training dataset to normalize the data. The second time, use the following values that were used for the ImageNet training :\n","\n"," ||__R__|__G__|__B__|\n"," |------|-------|-------|-------|\n"," | __Mean__ | 0.485 | 0.456 | 0.406 |\n"," | __Std__ | 0.229 | 0.224 | 0.225 |\n"," #### Data Normalization using the values of the training dataset\n"," As mentioned, for the first data normalization strategy, the values calculated based on the training dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mean_stg_1, std_stg_1 = phm.calculate_normalization_params(training_ds_dir)\n","\n","print('Training Dataset Calculated Normalization parameters:')\n","print('Mean: %s' % str(mean_stg_1))\n","print('Std: %s' % str(std_stg_1))\n","\n","norm_stg1_transform = trans.Compose([\n","    trans.Resize((224,224)),\n","    trans.ToTensor(),\n","    trans.Normalize(\n","        mean_stg_1, \n","        std_stg_1\n","    )\n","])\n","\n","norm_stg2_transform = trans.Compose([\n","    trans.Resize((224,224)),\n","    trans.ToTensor(),\n","    trans.Normalize(\n","        mean=(0.485, 0.456, 0.406),\n","        std=(0.229, 0.224, 0.225)\n","    )\n","])\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Neural Network Model Definition"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('Define Neural network model ...')\n","model = phm.create_model()\n","print(model)\n","\n","_, data_loader = phm.get_datasets(training_ds_dir, norm_stg1_transform)\n","\n","sample_data = next(iter(data_loader))\n","sample_data = sample_data[0].cuda() if phm.is_cuda_available() else sample_data[0]\n","out_test = model(sample_data)\n","make_dot(out_test.mean(), params=dict(model.named_parameters()))\n",""]},{"cell_type":"markdown","metadata":{},"source":[" The figure is a presentation of the model. If a node represents a backward function, it is gray. Otherwise, the node represents a tensor and is either blue, orange, or green. Blue: reachable leaf tensors that requires grad (tensors whose .grad fields will be populated during .backward()) Orange: saved tensors of custom autograd functions as well as those saved by built-in backward nodes Green: tensor passed in as outputs Dark green: if any output is a view, we represent its base tensor with a dark green node. <br>\n","\n"," |__Parameters__|__Values__|\n"," |--------------|----------|\n"," |__Epoch Number__|20|\n"," |__Batch Size__|68|\n"," |__Learning Rate__|0.1|\n"," |__Momentum__|0.9|\n"," ## Question 1-a : Using the default random initialization\n"," ### (a) Normalization using the values from dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","epoch = 31\n","batch_size = 68\n","learning_rate = 0.1\n","momentum = 0.9\n","\n","# Create Models\n","model = phm.create_model(pretrained=False)\n","\n","phm.run_experiment('q1_a_norm_data',\n","    model,\n","    norm_stg1_transform, \n","    training_dir=training_ds_dir,\n","    testing_dir=testing_ds_dir,\n","    learning_rate=learning_rate,\n","    batch_size=batch_size,\n","    num_epochs=epoch,\n","    momentum=momentum\n",")\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### (b) Normalization using ImageNet values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Create Models\n","model = phm.create_model(pretrained=False)\n","\n","phm.run_experiment('q1_a_norm_imagenet',\n","    model,\n","    norm_stg2_transform, \n","    training_dir=training_ds_dir,\n","    testing_dir=testing_ds_dir,\n","    learning_rate=learning_rate,\n","    batch_size=batch_size,\n","    num_epochs=epoch,\n","    momentum=momentum\n",")\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Question 1-b : Using the pre-trained model, but freezing all convolution parameters.\n"," ### (a) Normalization using the values from dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = phm.create_model(pretrained=True)\n","model = phm.freeze_conv_params(model)\n","\n","phm.run_experiment('q1_b_norm_data',\n","    model,\n","    norm_stg1_transform, \n","    training_dir=training_ds_dir,\n","    testing_dir=testing_ds_dir,\n","    learning_rate=learning_rate,\n","    batch_size=batch_size,\n","    num_epochs=epoch,\n","    momentum=momentum\n",")\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### (b) Normalization using ImageNet values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = phm.create_model(pretrained=True)\n","model = phm.freeze_conv_params(model)\n","\n","phm.run_experiment('q1_b_norm_imagenet',\n","    model,\n","    norm_stg2_transform, \n","    training_dir=training_ds_dir,\n","    testing_dir=testing_ds_dir,\n","    learning_rate=learning_rate,\n","    batch_size=batch_size,\n","    num_epochs=epoch,\n","    momentum=momentum\n",")\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Question 1-c : Using the pre-trained model, but only freezing the parameters in \"layer1\".\n"," ### (a) Normalization using the values from dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = phm.create_model(pretrained=True)\n","model = phm.freeze_layer1_params(model)\n","\n","phm.run_experiment('q1_c_norm_data',\n","    model,\n","    norm_stg1_transform, \n","    training_dir=training_ds_dir,\n","    testing_dir=testing_ds_dir,\n","    learning_rate=learning_rate,\n","    batch_size=batch_size,\n","    num_epochs=epoch,\n","    momentum=momentum\n",")\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### (b) Normalization using ImageNet values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = phm.create_model(pretrained=True)\n","model = phm.freeze_layer1_params(model)\n","\n","phm.run_experiment('q1_c_norm_imagenet',\n","    model,\n","    norm_stg2_transform, \n","    training_dir=training_ds_dir,\n","    testing_dir=testing_ds_dir,\n","    learning_rate=learning_rate,\n","    batch_size=batch_size,\n","    num_epochs=epoch,\n","    momentum=momentum\n",")\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Question 1-d : Using the pre-trained model, but letting all the parameters (including the convolution layers) be adjusted by backprop.\n"," ### (a) Normalization using the values from dataset"]},{"cell_type":"markdown","metadata":{},"source":[" ### (b) Normalization using ImageNet values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = phm.create_model(pretrained=True)\n","\n","phm.run_experiment('q1_d_norm_imagenet',\n","    model,\n","    norm_stg2_transform, \n","    training_dir=training_ds_dir,\n","    testing_dir=testing_ds_dir,\n","    learning_rate=learning_rate,\n","    batch_size=batch_size,\n","    num_epochs=epoch,\n","    momentum=momentum\n",")"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}